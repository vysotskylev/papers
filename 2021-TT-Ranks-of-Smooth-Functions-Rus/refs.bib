@article{Oseledets2009a,
abstract = {—We present a new tensor decomposition, which is nonrecursive, does not suffer from the curse of dimensionality, but has a viable stability properties and can be computed by a robust algorithm via a sequence of SVD decompositions. The new form gives a clear and conve-nient way to implement all basic operations efficiently. A fast recompression procedure is presented, as well as basic linear algebra operations.},
author = {Oseledets, Ivan},
file = {:home/levysotsky/Science/Tyrtyshnikov/Papers/B2L-C2-Oseledets-4181.pdf:pdf},
journal = {Preprint},
pages = {388--390},
title = {{Compact matrix form of the d-dimensional tensor decomposition}},
year = {2009}
}
@article{Oseledets2010,
abstract = {As is well known, a rank-r matrix can be recovered from a cross of r linearly independent columns and rows, and an arbitrary matrix can be interpolated on the cross entries. Other entries by this cross or pseudo-skeleton approximation are given with errors depending on the closeness of the matrix to a rank-r matrix and as well on the choice of cross. In this paper we extend this construction to d-dimensional arrays (tensors) and suggest a new interpolation formula in which a d-dimensional array is interpolated on the entries of some TT-cross (tensor train-cross). The total number of entries and the complexity of our interpolation algorithm depend on d linearly, so the approach does not suffer from the curse of dimensionality. We also propose a TT-cross method for computation of d-dimensional integrals and apply it to some examples with dimensionality in the range from d = 100 up to d = 4000 and the relative accuracy of order 10- 10. In all constructions we capitalize on the new tensor decomposition in the form of tensor trains (TT-decomposition). {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Oseledets, Ivan and Tyrtyshnikov, Eugene},
doi = {10.1016/j.laa.2009.07.024},
file = {:home/levysotsky/Science/Tyrtyshnikov/Papers/tt-cross-LAA10286.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
keywords = {Cross approximation,Curse of dimensionality,Interpolation,Low-rank matrices,Multi-way arrays,Multidimensional integration,Singular value decomposition,TT decomposition,Tensor decompositions,Tensor trains},
number = {1},
pages = {70--88},
title = {{TT-cross approximation for multidimensional arrays}},
volume = {432},
year = {2010}
}
@article{Oseledets2011,
abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low- rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
author = {Oseledets, Ivan},
doi = {10.1137/090752286},
file = {:home/levysotsky/Science/Tyrtyshnikov/Papers/oseledets2011.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {SVD,TT-format,high-dimensional problems,tensors},
mendeley-tags = {SVD,TT-format,high-dimensional problems,tensors},
month = {jan},
number = {5},
pages = {2295--2317},
title = {{Tensor-Train Decomposition}},
url = {http://epubs.siam.org/doi/10.1137/090752286},
volume = {33},
year = {2011}
}
@article{Khoromskij2011,
abstract = {In the present paper, we discuss the novel concept of super-compressed tensor-structured data formats in high-dimensional applications. We describe the multifolding or quantics-based tensor approximation method of O(dlog N)-complexity (logarithmic scaling in the volume size), applied to the discrete functions over the product index set {1,N}⊗d, or briefly N-d tensors of size Nd, and to the respective discretized differential-integral operators in ℝd. As the basic approximation result, we prove that a complex exponential sampled on an equispaced grid has quantics rank 1. Moreover, a Chebyshev polynomial, sampled over a Chebyshev Gauss-Lobatto grid, has separation rank 2 in the quantics tensor format, while for the polynomial of degree m over a Chebyshev grid the respective quantics rank is at most 2m+1. For N-d tensors generated by certain analytic functions, we give a constructive proof of the O(dlog Nlog $\epsilon$-1)-complexity bound for their approximation by low-rank 2-(dlog N) quantics tensors up to the accuracy $\epsilon$>0. In the case $\epsilon$=O(N-$\alpha$), $\alpha$>0, our approach leads to the quantics tensor numerical method in dimension d, with the nearly optimal asymptotic complexity O(d/$\alpha$log 2$\epsilon$-1). From numerical examples presented here, we observe that the quantics tensor method has proved its value in application to various function related tensors/matrices arising in computational quantum chemistry and in the traditional finite element method/boundary element method (FEM/BEM). The tool apparently works. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
author = {Khoromskij, Boris},
doi = {10.1007/s00365-011-9131-1},
file = {:home/levysotsky/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khoromskij - 2011 - O(dlog N)-Quantics Approximation of N-d Tensors in High-Dimensional Numerical Modeling.pdf:pdf},
issn = {0176-4276},
journal = {Constructive Approximation},
keywords = {FEM,High-dimensional problems,Material sciences,Matrix-valued functions,Quantics folding of vectors,Rank-structured tensor approximation,Stochastic modeling},
month = {oct},
number = {2},
pages = {257--280},
publisher = {Springer},
title = {{$O(d\log N)$-Quantics Approximation of $N$-d Tensors in High-Dimensional Numerical Modeling}},
url = {https://link.springer.com/article/10.1007/s00365-011-9131-1},
volume = {34},
year = {2011}
}
@article{Grasedyck2010,
abstract = {We analyze and characterize the possibility to represent or approximate tensors that stem from a tensorization of vectors, matrices, or tensors by low (hierarchi-cal) rank. Our main result is that for vectors that stem from the evaluation of a polynomial f of degree p on an equispaced grid, the (hierarchical) rank is bounded by 1 + p. This is not true for the canonical rank, and we prove this by a small counterexample. We extend our result to functions with (few) singu-larities that are otherwise analytic: for an asymptotically smooth function with m singularities the rank required to achieve a point-wise accuracy of $\epsilon$ is of the size k ≤ C + log 2 (1/$\epsilon$) + 2m. The storage requirements for a tensorized vector of length n are O(log(n)k 3) and arithmetic operations (e.g., truncated addition) in the format are of O(log(n)k 4) complexity.},
author = {Grasedyck, Lars},
file = {:home/levysotsky/Science/Tyrtyshnikov/Papers/IGPM308_k.pdf:pdf},
isbn = {4924180939},
journal = {Preprint},
keywords = {hierarchical tucker,tensor approximation,tensor rank,tensor train,tensorization,tt},
number = {308},
title = {{Polynomial Approximation in Hierarchical Tucker Format by Vector-Tensorization}},
url = {http://www.dfg-spp1324.de/download/preprints/preprint043.pdf},
year = {2010}
}
@article{Oseledets2013,
abstract = {In this paper, we obtain explicit representations of several multivariate functions in the Tensor Train (TT) format and explicit TT-representations of tensors that stem from the tensorization of univariate functions on grids. Previous results contained only estimates on the number of parameters (tensor ranks), and this paper fills this gap by providing explicit low-parametric representations for these functions and tensors. {\textcopyright} 2012 Springer Science+Business Media New York.},
author = {Oseledets, Ivan},
doi = {10.1007/s00365-012-9175-x},
file = {:home/levysotsky/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oseledets - 2013 - Constructive Representation of Functions in Low-Rank Tensor Formats(2).pdf:pdf},
issn = {0176-4276},
journal = {Constructive Approximation},
keywords = {Explicit representations,Multivariate functions,QTT-format,TT-format,Tensor decompositions},
month = {feb},
number = {1},
pages = {1--18},
publisher = {Springer},
title = {{Constructive Representation of Functions in Low-Rank Tensor Formats}},
url = {http://link.springer.com/10.1007/s00365-012-9175-x},
volume = {37},
year = {2013}
}
@article{Oseledets2009,
abstract = {A new method for structured representation of matrices and vectors is presented. The method is based on the representation of a matrix as a d-dimensional tensor and applying the TT-decomposition proposed recently. It turned out that for many important cases the number of parameters to represent an n × n matrix falls down to O(log$\alpha$ n), giving a logarithmic storage. It is shown that this format can be used not only for storage reduction, but also for linear algebra operations. Possible applications include differential and integral equations, and data and image compression. Copyright {\textcopyright} 2010 Society for Industrial and Applied Mathematics.},
author = {Oseledets, Ivan},
doi = {10.1137/090757861},
issn = {08954798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {Complexity,Logarithmic,TT-format,Tensors},
month = {jun},
number = {4},
pages = {2130--2145},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Approximation of $2^d \times 2^d$ matrices using tensor decomposition}},
volume = {31},
year = {2009}
}
@article{Tyrtyshnikov2003,
author = {Tyrtyshnikov, E E},
doi = {10.1070/SM2003v194n06ABEH000747},
issn = {1064-5616},
journal = {Sbornik: Mathematics},
month = {jun},
number = {6},
pages = {941--954},
publisher = {Turpion-Moscow Limited},
title = {{Tensor approximations of matrices generated by asymptotically smooth functions}},
url = {http://stacks.iop.org/1064-5616/194/i=6/a=A09?key=crossref.759fa240378703afa76f11293a94de57},
volume = {194},
year = {2003}
}
@article{Eckart1936,
abstract = {Transactivation-defective retinoid X and thyroid hormone receptors have been used to examine mechanisms of hormonal activation. Activation and repression of transcription by retinoid X and thyroid hormone receptors are shown to be mediated by physically distinct and functionally independent regions of the hormone binding domain. Nevertheless, the ability of receptors to respond to hormone requires communication between both functional domains. Deletion of the hormone-dependent transactivation function of the retinoid X receptor, the common subunit of heterodimeric nuclear receptors, significantly impairs hormone-dependent transcription by retinoic acid, thyroid hormone, and vitamin D receptors. The results indicate that receptors do not exist in static off and on conformations but that hormone alters an equilibrium between inactive and active states.},
author = {Eckart, Carl and Young, Gale},
doi = {10.1007/BF02288367},
file = {:home/levysotsky/Science/Tyrtyshnikov/Papers/eckart&young.1936.pdf:pdf},
issn = {0033-3123},
journal = {Psychometrika},
month = {sep},
number = {3},
pages = {211--218},
title = {{The approximation of one matrix by another of lower rank}},
url = {http://link.springer.com/10.1007/BF02288367},
volume = {1},
year = {1936}
}
@inproceedings{Vysotsky2020,
abstract = {Discretization followed by tensorization (mapping from low-dimensional to high-dimensional data) can be used to construct low-parametric approximations of functions. For example, a function f defined on [0, 1] may be mapped to a d-dimensional tensor with elements. The tensor A can now be compressed using one of the tensor formats, e.g. tensor train format. It has been noticed in practice that approximate TT-ranks of tensorizations of degree-n polynomials grow very slowly with respect to n, while the only known bound for them is. In this paper we try to explain the observed effect. New bounds of the described TT-ranks are proved and shown experimentally to quite successfully capture the observed distribution of ranks.},
author = {Vysotsky, Lev},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-030-41032-2_21},
isbn = {9783030410315},
issn = {16113349},
keywords = {Discretization,TT-decomposition,TT-ranks,Tensor train format,Tensorization},
month = {jun},
pages = {189--196},
publisher = {Springer},
title = {{On Tensor-Train Ranks of Tensorized Polynomials}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-41032-2_21},
volume = {11958 LNCS},
year = {2020}
}
@book{MNA,
address = {Москва},
author = {Тыртышников, Евгений Евгеньевич},
file = {:home/levysotsky/Teaching/Msu/mna.pdf:pdf},
isbn = {978-5-7695-3925-1},
publisher = {Академия},
title = {{Методы численного анализа}},
year = {2007}
}
